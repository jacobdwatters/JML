<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>LossGradients.java</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">java-machine-learning</a> &gt; <a href="index.source.html" class="el_package">com.jml.losses</a> &gt; <span class="el_source">LossGradients.java</span></div><h1>LossGradients.java</h1><pre class="source lang-java linenums">package com.jml.losses;

import com.jml.core.Model;
import linalg.Matrix;

public class LossGradients {

    // A private constructor to hide the implicit constructor.
<span class="nc" id="L9">    private LossGradients() {</span>
<span class="nc" id="L10">        throw new IllegalStateException(&quot;Utility class, Can not create instantiated.&quot;);</span>
    }
<span class="nc" id="L12">    static double h = 0.5e-8;</span>


    /**
     * Gradient of the {@link LossFunctions#sse sse} function for a Linear, MultiLinear, or polynomial regression model.
     */
<span class="nc" id="L18">    public static Function sseLinRegGrad = (Matrix w, Matrix X, Matrix y, Model model) -&gt; {</span>
<span class="nc" id="L19">        return (X.T()).mult(X).mult(w).sub(X.T().mult(y));</span>
    };



<span class="nc" id="L24">    public static Function sseGrad = (Matrix w, Matrix X, Matrix y, Model model) -&gt; {</span>

<span class="nc" id="L26">        Matrix grad = new Matrix(w.shape());</span>

        // A diagonal matrix containing the value of h along the diagonal.
<span class="nc" id="L29">        Matrix H = Matrix.I(w.numRows()).scalMult(h);</span>

<span class="nc bnc" id="L31" title="All 2 branches missed.">        for(int i=0; i&lt;w.numRows(); i++) { // Compute partial derivative for each w_i in w</span>

<span class="nc" id="L33">            Matrix partial = LossFunctions.sse.compute(</span>
<span class="nc" id="L34">                    w.add(H.getColAsVector(i)), X, y, model).sub(</span>
<span class="nc" id="L35">                        LossFunctions.sse.compute(w, X, y, model)</span>
<span class="nc" id="L36">                    ).scalDiv(h);</span>

            // Set the gradient at the given index to be the computed partial derivative.
<span class="nc" id="L39">            grad.set(partial.getAsDouble(0, 0), i, 0);</span>
        }

<span class="nc" id="L42">        System.out.println(&quot;\n\ngrad:\n&quot; + grad.toString() + &quot;\n\n&quot;);</span>

<span class="nc" id="L44">        return grad;</span>
    };

}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.7.202105040129</span></div></body></html>