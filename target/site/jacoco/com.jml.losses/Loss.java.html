<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>Loss.java</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">java-machine-learning</a> &gt; <a href="index.source.html" class="el_package">com.jml.losses</a> &gt; <span class="el_source">Loss.java</span></div><h1>Loss.java</h1><pre class="source lang-java linenums">package com.jml.losses;


import linalg.Matrix;

/**
 * This class contains methods for computing the loss between two datasets stored in double[][] arrays.
 */
<span class="nc" id="L9">public abstract class Loss {</span>


    /**
     * Computes the of mean sum of squared-errors loss function.&lt;br&gt;
     * That is &lt;code&gt;mse = (1/n)*sum(yPred&lt;sub&gt;i&lt;/sub&gt; - y&lt;sub&gt;i&lt;/sub&gt;)&lt;sup&gt;2&lt;/sup&gt;&lt;/code&gt;.
     *
     * @param y Expected value.
     * @param yPred Predicted value.
     * @return Mean squared-error loss of y and yPred.
     */
    public static double mse(double[][] y, double[][] yPred) {
<span class="nc" id="L21">        return LossFunctions.mse.compute(new Matrix(y), new Matrix(yPred)).getAsDouble(0, 0);</span>
    }


    /**
     * Computes the of sum of squared-errors loss function.&lt;br&gt;
     * That is &lt;code&gt;sse = sum(x&lt;sub&gt;i&lt;/sub&gt; - y&lt;sub&gt;i&lt;/sub&gt;)&lt;sup&gt;2&lt;/sup&gt;&lt;/code&gt;.
     *
     * @param y Expected value.
     * @param yPred Predicted value.
     * @return Squared-error loss of y and yPred.
     */
    public static double sse(double[][] y, double[][] yPred) {
<span class="nc" id="L34">        return LossFunctions.sse.compute(new Matrix(y), new Matrix(yPred)).getAsDouble(0, 0);</span>
    }


    /**
     * The binary cross-entropy loss function. bce = -(1/n) * sum( y*&lt;sub&gt;i&lt;/sub&gt;ln(yPred&lt;sub&gt;i&lt;/sub&gt;) + (1-y&lt;sub&gt;i&lt;/sub&gt;)*ln(yPred&lt;sub&gt;i&lt;/sub&gt;) )&lt;br&gt;&lt;br&gt;
     * Note: binary cross-entropy is undefined for p=0 or p=1, so probabilities adjusted to be &quot;very close&quot; to 0 or 1 if
     * appropriate.
     *
     * @param y Expected value.
     * @param yPred Predicted value.
     * @return Squared-error loss of y and yPred.
     */
    public static double binCrossEntropy(double[][] y, double[][] yPred) {
<span class="nc" id="L48">        return LossFunctions.binCrossEntropy.compute(new Matrix(y), new Matrix(yPred)).getAsDouble(0, 0);</span>
    }


    /**
     * The binary cross-entropy loss function. ce = -(1/n) * sum( y*&lt;sub&gt;i&lt;/sub&gt;ln(yPred&lt;sub&gt;i&lt;/sub&gt;) )&lt;br&gt;&lt;br&gt;
     * Note: cross-entropy is undefined for p=0 or p=1, so probabilities adjusted to be &quot;very close&quot; to 0 or 1 if
     * appropriate.
     *
     * @param y Expected value.
     * @param yPred Predicted value.
     * @return Squared-error loss of y and yPred.
     */
    public static double crossEntropy(double[][] y, double[][] yPred) {
<span class="nc" id="L62">        return LossFunctions.crossEntropy.compute(new Matrix(y), new Matrix(yPred)).getAsDouble(0, 0);</span>
    }
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.7.202105040129</span></div></body></html>