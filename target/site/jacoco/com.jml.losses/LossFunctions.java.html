<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>LossFunctions.java</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">java-machine-learning</a> &gt; <a href="index.source.html" class="el_package">com.jml.losses</a> &gt; <span class="el_source">LossFunctions.java</span></div><h1>LossFunctions.java</h1><pre class="source lang-java linenums">package com.jml.losses;

import linalg.Matrix;

/**
 * This class contains lambda functions for various loss functions including:
 * &lt;pre&gt;
 *     - {@link #sse}: sum of squared-errors loss.
 *     - {@link #binCrossEntropy}: binary cross-entropy loss (Cross-entropy for two classes).
 *     - {@link #crossEntropy}: cross-entropy loss (For multiple classes).
 * &lt;/pre&gt;
 */
public abstract class LossFunctions {

    /* TODO: All loss functions should become an object rather than a lambda so that we can define, compute() and slope() function
        for each loss. This will allow different loss functions to be specified for a NeuralNetwork since function and its
        derivative needs to be computed.
     */

    // A private constructor to hide the implicit constructor.
<span class="nc" id="L21">    private LossFunctions() {</span>
<span class="nc" id="L22">        throw new IllegalStateException(&quot;Utility class, Can not create instantiated.&quot;);</span>
    }


    /**
     * The sum of mean squared-errors loss function.&lt;br&gt;
     * That is &lt;code&gt;mse = (1/n)*sum(x&lt;sub&gt;i&lt;/sub&gt; - y&lt;sub&gt;i&lt;/sub&gt;)&lt;sup&gt;2&lt;/sup&gt;&lt;/code&gt;
     * where &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are datasets of length &lt;code&gt;n&lt;/code&gt;,
     * and &lt;code&gt;x&lt;/code&gt; is the actual data and &lt;code&gt;y&lt;/code&gt; is the predicted data.
     */
<span class="fc" id="L32">    public static final Function mse = (Matrix y, Matrix yPred) -&gt; yPred.sub(y).T().mult(yPred.sub(y)).scalDiv(y.numRows());</span>


    /**
     * The sum of squared-errors loss function.&lt;br&gt;
     * That is &lt;code&gt;sse = sum(x&lt;sub&gt;i&lt;/sub&gt; - y&lt;sub&gt;i&lt;/sub&gt;)&lt;sup&gt;2&lt;/sup&gt;&lt;/code&gt;
     * where &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;code/&gt; are datasets of length &lt;code&gt;n&lt;/code&gt;,
     * and &lt;code&gt;x&lt;/code&gt; is the actual data and &lt;code&gt;y&lt;/code&gt; is the predicted data.
     */
<span class="fc" id="L41">    public static final Function sse = (Matrix y, Matrix yPred) -&gt; yPred.sub(y).T().mult(yPred.sub(y));</span>


    /**
     * The binary cross-entropy loss function.&lt;br&gt;
     * bce = -(1/n) * sum( y*&lt;sub&gt;i&lt;/sub&gt;ln(yPred&lt;sub&gt;i&lt;/sub&gt;) + (1-y&lt;sub&gt;i&lt;/sub&gt;)*ln(yPred&lt;sub&gt;i&lt;/sub&gt;) )&lt;br&gt;&lt;br&gt;
     * Note: cross-entropy is undefined for p=0 or p=1, so probabilities adjusted to be &quot;very close&quot; to 0 or 1 if
     * appropriate.
     */
<span class="fc" id="L50">    public static final Function binCrossEntropy = (Matrix y, Matrix yPred) -&gt; {</span>
<span class="fc" id="L51">        double eps = 1e-15;</span>
<span class="fc" id="L52">        double loss = 0;</span>
<span class="fc" id="L53">        Matrix result = new Matrix(1);</span>

<span class="fc bfc" id="L55" title="All 2 branches covered.">        if(yPred.numCols() &gt; 2) {</span>
<span class="fc" id="L56">            throw new IllegalArgumentException(&quot;Predictions seem to have more than two classes. Consider &quot; +</span>
                    &quot;crossEntropy() for multiple classes.&quot;);
        }

<span class="fc bfc" id="L60" title="All 2 branches covered.">        for(int i=0; i&lt;yPred.numRows(); i++) {</span>
            // cross-entropy is undefined for p=0 or p=1, so probabilities are clipped to max(eps, min(1 - eps, p)).
<span class="fc bfc" id="L62" title="All 2 branches covered.">            if(yPred.getAsDouble(i, 0)==0) {</span>
<span class="fc" id="L63">                yPred.set(eps, i,  0);</span>
<span class="fc bfc" id="L64" title="All 2 branches covered.">            } else if(yPred.getAsDouble(i, 0)==1) {</span>
<span class="fc" id="L65">                yPred.set(1-eps, i,  0);</span>
            }

<span class="fc" id="L68">            loss += y.getAsDouble(i, 0)*Math.log(yPred.getAsDouble(i, 0))</span>
<span class="fc" id="L69">                    + (1-y.getAsDouble(i, 0))*Math.log(1-yPred.getAsDouble(i, 0));</span>
        }

<span class="fc" id="L72">        result.set(-loss/yPred.numRows(), 0,0);</span>

<span class="fc" id="L74">        return result;</span>
    };


    /**
     * the cross-entropy loss function.&lt;br&gt;
     * ce = -(1/n) * sum( y*&lt;sub&gt;i&lt;/sub&gt;ln(yPred&lt;sub&gt;i&lt;/sub&gt;) )&lt;br&gt;&lt;br&gt;
     * Note: cross-entropy is undefined for p=0 or p=1, so probabilities adjusted to be &quot;very close&quot; to 0 or 1 if
     * appropriate.
     */
<span class="fc" id="L84">    public static final Function crossEntropy = (Matrix y, Matrix yPred) -&gt; {</span>
<span class="fc" id="L85">        double eps = 1e-15;</span>
<span class="fc" id="L86">        double loss = 0;</span>
<span class="fc" id="L87">        Matrix result = new Matrix(1);</span>

        // y contains the actual labels as a one-hot vector.
<span class="fc bfc" id="L90" title="All 2 branches covered.">        for(int i=0; i&lt;yPred.numRows(); i++) {</span>
<span class="fc bfc" id="L91" title="All 2 branches covered.">            for(int j=0; j&lt;yPred.numCols(); j++) {</span>
                // cross-entropy is undefined for p=0 and p=1, so probabilities are clipped to max(eps, min(1 - eps, p)).
<span class="fc bfc" id="L93" title="All 2 branches covered.">                if(yPred.getAsDouble(i, j)==0) {</span>
<span class="fc" id="L94">                    yPred.set(eps, i,  j);</span>
<span class="fc bfc" id="L95" title="All 2 branches covered.">                } else if(yPred.getAsDouble(i, j)==1) {</span>
<span class="fc" id="L96">                    yPred.set(1-eps, i,  j);</span>
                }

<span class="fc bfc" id="L99" title="All 2 branches covered.">                if(y.getAsDouble(i, j) == 1) {</span>
<span class="fc" id="L100">                    loss += Math.log(yPred.getAsDouble(i, j));</span>
<span class="pc bpc" id="L101" title="1 of 2 branches missed.">                } else if(y.getAsDouble(i, j) != 0) {</span>
<span class="nc" id="L102">                    throw new IllegalArgumentException(&quot;y does not seem to be a one-hot vector. Must contain binary entries only.&quot;);</span>
                }
            }
        }

<span class="fc" id="L107">        result.set(-loss/yPred.numRows(), 0,0);</span>

<span class="fc" id="L109">        return result;</span>
    };
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.7.202105040129</span></div></body></html>