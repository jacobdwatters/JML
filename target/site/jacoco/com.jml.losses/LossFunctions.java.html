<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>LossFunctions.java</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">java-machine-learning</a> &gt; <a href="index.source.html" class="el_package">com.jml.losses</a> &gt; <span class="el_source">LossFunctions.java</span></div><h1>LossFunctions.java</h1><pre class="source lang-java linenums">package com.jml.losses;

import com.jml.clasifiers.LogisticRegression;
import com.jml.core.Model;
import linalg.Matrix;
import linalg.Vector;

/**
 * This class contains lambda functions for various loss functions including:
 * &lt;pre&gt;
 *     - {@link #sse}: sum of squared-errors loss.
 *     - {@link #binCrossEntropy}: binary cross-entropy loss (Cross-entropy for two classes).
 *     - {@link #crossEntropy}: cross-entropy loss (For multiple classes).
 * &lt;/pre&gt;
 */
public class LossFunctions {

    // A private constructor to hide the implicit constructor.
<span class="nc" id="L19">    private LossFunctions() {</span>
<span class="nc" id="L20">        throw new IllegalStateException(&quot;Utility class, Can not create instantiated.&quot;);</span>
    }

    /* TODO: Replace model parameter with predictions parameter. Then these losses can be computed without needing
        to specify a model. Also, then the weight matrix and the feature matrix do not need to be passed as parameters */

    /**
     * The sum of mean squared-errors loss function.&lt;br&gt;
     * That is &lt;code&gt;mse = (1/n)sum(x&lt;sub&gt;i&lt;/sub&gt; - y&lt;sub&gt;i&lt;/sub&gt;)&lt;sup&gt;2&lt;/sup&gt;&lt;/code&gt;
     * where &lt;code&gt;x&lt;code/&gt; and &lt;code&gt;y&lt;code/&gt; are datasets of length &lt;code&gt;n&lt;code/&gt;,
     * and &lt;code&gt;x&lt;code/&gt; is the actual data and &lt;code&gt;y&lt;code/&gt; is the predicted data.
     *
     * @param w
     * @param X
     * @param y
     * @param model
     */
<span class="fc" id="L37">    public static Function mse = (Matrix y, Matrix yPred) -&gt; {</span>
<span class="nc" id="L38">        return yPred.sub(y).T().mult(yPred.sub(y)).scalDiv(y.numRows());</span>
    };


    /**
     * The sum of squared-errors loss function.&lt;br&gt;
     * That is &lt;code&gt;sse = sum(x&lt;sub&gt;i&lt;/sub&gt; - y&lt;sub&gt;i&lt;/sub&gt;)&lt;sup&gt;2&lt;/sup&gt;&lt;/code&gt;
     * where &lt;code&gt;x&lt;code/&gt; and &lt;code&gt;y&lt;code/&gt; are datasets of length &lt;code&gt;n&lt;code/&gt;,
     * and &lt;code&gt;x&lt;code/&gt; is the actual data and &lt;code&gt;y&lt;code/&gt; is the predicted data.
     *
     *
     * @param w
     * @param X
     * @param y
     * @param model
     */
<span class="fc" id="L54">    public static Function sse = (Matrix y, Matrix yPred) -&gt; {</span>
<span class="fc" id="L55">        return yPred.sub(y).T().mult(yPred.sub(y));</span>
    };


    /**
     * The binary cross-entropy loss function.
     *
     * @param w
     * @param X
     * @param y
     * @param model
     */
<span class="fc" id="L67">    public static Function binCrossEntropy = (Matrix y, Matrix yPred) -&gt; {</span>
<span class="nc" id="L68">        double eps = 1e-15;</span>
<span class="nc" id="L69">        double loss = 0;</span>
<span class="nc" id="L70">        Matrix result = new Matrix(1);</span>

<span class="nc bnc" id="L72" title="All 2 branches missed.">        if(yPred.numCols() &gt; 2) {</span>
<span class="nc" id="L73">            throw new IllegalArgumentException(&quot;Predictions seem to have more than two classes. Consider &quot; +</span>
                    &quot;crossEntropy() for multiple classes.&quot;);
        }

<span class="nc bnc" id="L77" title="All 2 branches missed.">        for(int i=0; i&lt;yPred.numRows(); i++) {</span>
            // cross-entropy is undefined for p=0 or p=1, so probabilities are clipped to max(eps, min(1 - eps, p)).
<span class="nc bnc" id="L79" title="All 2 branches missed.">            if(yPred.getAsDouble(i, 0)==0) {</span>
<span class="nc" id="L80">                yPred.set(eps, i,  0);</span>
<span class="nc bnc" id="L81" title="All 2 branches missed.">            } else if(yPred.getAsDouble(i, 0)==1) {</span>
<span class="nc" id="L82">                yPred.set(1-eps, i,  0);</span>
            }

<span class="nc" id="L85">            loss += y.getAsDouble(i, 0)*Math.log(yPred.getAsDouble(i, 0))</span>
<span class="nc" id="L86">                    + (1-y.getAsDouble(i, 0))*Math.log(1-yPred.getAsDouble(i, 0));</span>
        }

<span class="nc" id="L89">        result.set(-loss/yPred.numRows(), 0,0);</span>

<span class="nc" id="L91">        return result;</span>
    };


    /**
     * the cross-entropy loss function.
     * Note: cross-entropy is undefined for p=0 or p=1, so probabilities adjusted to be &quot;very close&quot; to 0 or 1 if
     * appropriate.
     *
     * @param w
     * @param X
     * @param y
     * @param model
     */
<span class="fc" id="L105">    public static Function crossEntropy = (Matrix y, Matrix yPred) -&gt; {</span>
<span class="nc" id="L106">        double eps = 1e-15;</span>
<span class="nc" id="L107">        double loss = 0;</span>
<span class="nc" id="L108">        Matrix result = new Matrix(1);</span>
        // y contains the actual labels as a one-hot vector.

<span class="nc bnc" id="L111" title="All 2 branches missed.">        for(int i=0; i&lt;yPred.numRows(); i++) {</span>
<span class="nc bnc" id="L112" title="All 2 branches missed.">            for(int j=0; j&lt;yPred.numCols(); j++) {</span>
                // cross-entropy is undefined for p=0 or p=1, so probabilities are clipped to max(eps, min(1 - eps, p)).
<span class="nc bnc" id="L114" title="All 2 branches missed.">                if(yPred.getAsDouble(i, j)==0) {</span>
<span class="nc" id="L115">                    yPred.set(eps, i,  j);</span>
<span class="nc bnc" id="L116" title="All 2 branches missed.">                } else if(yPred.getAsDouble(i, j)==1) {</span>
<span class="nc" id="L117">                    yPred.set(1-eps, i,  j);</span>
                }

<span class="nc" id="L120">                loss += y.getAsDouble(i, j)*Math.log(yPred.getAsDouble(i, j));</span>
            }
        }

<span class="nc" id="L124">        result.set(-loss/yPred.numRows(), 0,0);</span>

<span class="nc" id="L126">        return result;</span>
    };
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.7.202105040129</span></div></body></html>