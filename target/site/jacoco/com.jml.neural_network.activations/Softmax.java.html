<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>Softmax.java</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">java-machine-learning</a> &gt; <a href="index.source.html" class="el_package">com.jml.neural_network.activations</a> &gt; <span class="el_source">Softmax.java</span></div><h1>Softmax.java</h1><pre class="source lang-java linenums">package com.jml.neural_network.activations;

import linalg.Matrix;
import linalg.Vector;


/**
 * The softmax activation function. f(&lt;b&gt;x&lt;/b&gt;)&lt;sub&gt;i&lt;/sub&gt; = exp(&lt;b&gt;x&lt;/b&gt;_i) / sum&lt;sub&gt;j=1&lt;/sub&gt;&lt;sup&gt;m&lt;/sup&gt;( exp(&lt;b&gt;x&lt;/b&gt;&lt;sub&gt;j&lt;/sub&gt;) ) where
 * &lt;b&gt;x&lt;/b&gt; is a vector of length m and sum&lt;sub&gt;j=1&lt;/sub&gt;&lt;sup&gt;m&lt;/sup&gt; ( exp(&lt;b&gt;x&lt;/b&gt;&lt;sub&gt;j&lt;/sub&gt;) ) = exp(&lt;b&gt;x&lt;/b&gt;&lt;sub&gt;1&lt;/sub&gt;) + exp(&lt;b&gt;x&lt;/b&gt;&lt;sub&gt;2&lt;/sub&gt;) + ... + exp(&lt;b&gt;x&lt;/b&gt;&lt;sub&gt;m&lt;/sub&gt;).
 */
<span class="fc" id="L11">public class Softmax implements ActivationFunction {</span>

<span class="fc" id="L13">    public final String NAME = &quot;Softmax&quot;;</span>
<span class="fc" id="L14">    private Matrix forwardIn = null, forwardOut = null;</span>

    /**
     * Applies the activation function, element-wise, to a matrix.
     *
     * @param data The matrix to apply activation function to.
     * @return The result of the element-wise activation function applied to the matrix.
     */
    @Override
    public Matrix forward(Matrix data) {
<span class="nc bnc" id="L24" title="All 2 branches missed.">        if(data.numCols()&gt;1) {</span>
<span class="nc" id="L25">            throw new IllegalArgumentException(&quot;Expecting column matrix for &quot; + NAME + &quot; activation but got shape &quot; + data.shape());</span>
        }

        // TODO: For this to work, each layer will need to be passed new Softmax() and NOT Activations.softmax as this will then
        //      be shared for all layers. Thus the Activations class should be removed entirely so that the user can not do this.
<span class="nc" id="L30">        this.forwardIn = data.copy();</span>

<span class="nc" id="L32">        double[] result = new double[data.numRows()];</span>
<span class="nc" id="L33">        double sum = 0;</span>
<span class="nc" id="L34">        double max = data.maxReal(); // Compute shift for numerical stability</span>

<span class="nc bnc" id="L36" title="All 2 branches missed.">        for(int i=0; i&lt;data.numRows(); i++) { // Compute denominator.</span>
<span class="nc" id="L37">            sum += Math.exp(data.getAsDouble(i, 0) - max);</span>
        }

<span class="nc bnc" id="L40" title="All 2 branches missed.">        for(int i=0; i&lt;data.numRows(); i++) { // Compute each entry for the resulting vector.</span>
<span class="nc" id="L41">            result[i] = Math.exp(data.getAsDouble(i, 0) - max) / sum;</span>
        }

<span class="nc" id="L44">        this.forwardOut = new Vector(result);</span>

<span class="nc" id="L46">        return this.forwardOut; // Return Softmax activation result as a column vector.</span>
    }

    /**
     * Applies the derivative of the activation function, element-wise, to a matrix.
     *
     * @param data The matrix to apply the derivative of the activation function to.
     * @return The slope of the activation function, evaluated element-wise, of the data matrix.
     */
    @Override
    public Matrix back(Matrix data) {

<span class="nc" id="L58">        Matrix softmax = this.forwardOut.copy().T();</span>
<span class="nc" id="L59">        Matrix grad = data.copy().T();</span>
<span class="nc" id="L60">        Matrix diag = toDiag(softmax.getValuesAsDouble()[0]);</span>
<span class="nc" id="L61">        Matrix dSoftmax = diag.sub(softmax.T().mult(softmax));</span>

<span class="nc" id="L63">        return grad.mult(dSoftmax).flatten(1);</span>
    }

    private Matrix toDiag(double[] values) {
<span class="nc" id="L67">        double[][] result = new double[values.length][values.length];</span>

<span class="nc bnc" id="L69" title="All 2 branches missed.">        for(int i = 0; i&lt;values.length; i++) {</span>
<span class="nc" id="L70">            result[i][i] = values[i];</span>
        }

<span class="nc" id="L73">        return new Matrix(result);</span>
    }

    /**
     * Gets the name of the activation function.
     *
     * @return The name of the activation function as a String.
     */
    @Override
    public String getName() {
<span class="nc" id="L83">        return this.NAME;</span>
    }
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.7.202105040129</span></div></body></html>